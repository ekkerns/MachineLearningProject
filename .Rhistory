mean(y)
traceback()
lm(y~x)
traceback()
debug(lm)
lm(Y~x)
#Set Up
install.packages("swirl")
library("swirl")
install_from_swirl("R Programming")
swirl()
head(flags)
dim(flags)
class(flags)
cls_lists<-lapply(flags, class)
cls_list<-lapply(flags, class)
cls_list
class(cls_list)
as.character(cls_list)
cls_lists<-sapply(flags, class)
cls_vect<-sapply(flags, class)
class(cls_vect)
sum(flags$orange)
flag_colors<-glags[,11:17]
flag_colors<-flags[,11:17]
head(flag_colors)
lapply(flag_colors,sum)
sapply(flag_colors,sum)
sapply(flag_colors,mean)
flag_shapes <- flags[, 19:23]
lapply(flag_shapes,range)
shape_mat<-sapply(flag_shapes,range)
shape_mat
class(shape_mat)
unique(c(3, 4, 5,
| 5, 5, 6, 6))
unique(c(3, 4, 5,5, 5, 6, 6))
uniques_vals<-lapply(flags,unique)
unique_vals<-lapply(flags,unique)
unique_vals
lapply(unique_vals,length)
sapply(unique_vals,length)
unique_vals<-sapply(flags,unique)
sapply(flags,unique)
lapply(unique_vals, function(elem) elem[2])
sapply(flags, unique)
vapply(flags, unique, numeric(1))
ok()
sapply(flags, class)
vapply(flags, class, character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate,flags$landmass, mean)
tapply(flags$population,flags$res, mean)
tapply(flags$population,flags$red, summary)
tapply(flags$population,flags$landmass, summary)
#Question 1
library(datasets)
data(iris)
tapply(iris$Sepal.Length,iris$Species,mean)
colMeans(iris)
lapply(iris[,1:4],1,mean)
lapply(iris,1,mean)
rowMeans(iris[,1:4])
colMeans(iris)
colMeans(iris[,1:4])
apply(iris,1,mean)
apply(iris[,1:4],1,mean)
apply(iris[,1:4],2,mean)
library(datasets)
data(mtcars)
?mtcars
with(mtcars,tapply(mpg,cyl,mean))
tapply(mtcars$mpg,mtcars$cyl,mean)
sapply(split(mtcars$mpg,mtcars$cyl),mean)
sapply(split(mtcars$hp,mtcars$cyl),mean)
makeCacheMatrix <- function(x = matrix()) {
inverse<- NULL
set <-function(y){
x<<-y
inverse<<-NULL
}
get <-function() x
setinverse <-function(solve) inverse<<- solve
getinverse <-function() inverse
list(set=set, get=get, setinverse=setinverse, getinverse=getinverse)
}
## checks to see if the inverse has already been made, if so, returns it, if not, calculates it
cacheSolve <- function(x, ...) {
inverse<-x$getinverse()
if(!is.null(inverse)){
message("getting pre-calculated inverse")
return(inverse)
}
data<-x$get()
inverse<-solve(data)
x$setinverse(inverse)
inverse
}
cacheSolve(mtcars)
traceback()
z<-matrix(5,2)
cacheSolve(z)
makeCahceMatrix(z)
makeCacheMatrix(z)
cacheSolve(z)
z<-matrix(2,2)
makeCacheMatrix(z)
cacheSolve(z)
Z<-makeCacheMatrix(z)
cacheSolve(z)
traceback()
debug(cacheSolve)
cacheSolve(z)
next()
makeCacheMatrix(makeCacheMatrix(z))
cacheSolve(makeCacheMatrix(z))
Q
getOptions(repos)
getOption("repos")
remove.packages("rmarkdown")
install.packages("rmarkdown")
remove.packages("rmarkdown")
install.packages("path_to_file"https://cran.r-project.org/src/contrib/rmarkdown_1.8.tar.gz"", repos = NULL, type="source")
install.packages("https://cran.r-project.org/src/contrib/rmarkdown_1.8.tar.gz", repos = NULL, type="source")
install.packages("C:\\Users\\ekkerns\\Downloads\\rmarkdown_1.8.zip\\rmarkdown\\R\\rmarkdown", repos = NULL, type="source")
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR() # this will start the updating process of your R installation.  It will check for newer versions, and if one is available, will guide you through the decisions you'd need to make.
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR() # this will start the updating process of your R installation.  It will check for newer versions, and if one is available, will guide you through the decisions you'd need to make.
detach("package:installr", unload=TRUE)
install.packages("rmarkdown")
library(datasets)
data("airquality")
summary(airquality)
pairs(airquality)
install.packages("slidify")
#Echo makes code not appear in HTML
#Rcode
time<-format(Sys.time(),"%a %b %X %Y")
rand<-rorm(1)
library(swirl)
swirl()
install_from_swirl("Statistical Inference")
swirl()
pause
0
library(swirl)
install_from_swirl("Statistical Inference")
swirl()
mplot(2)
myplot(2)
myplot(20)
myplot2(2)
qt(.975,2)
myplot2(20)
sleep
range(g1)
range(g2)
difference<-g1-g2
difference<-g2-g1
mean(difference)
s<-sd(difference)
mn+c(-1,1)*qt(0.975,9)*s/sqt(10)
mn+c(-1,1)*qt(0.975,9)*s/sqrt(10)
t.test(mn,9)$conf.int
t.test(difference),9)$conf.int
t.test(difference,9)$conf.int
t.test(difference)$conf.int
sp<-(8-1)*(15.34)^2+(21-1)*(18.23)^2
ns<-21+8-2
sp<-sqrt(sp/ns)
132.86-127.44+c(-1,1)*qt(0.975,ns)*sp*sqrt(1/8+1/21)
sp<-(9*var(g1)+9*var(g2))/18
sp<-sqrt((9*var(g1)+9*var(g2))/18)
md+c(-1,1)*qt(0.975,18)*sp*sqrt(1/10+1/10)
t.test(g2,g1,paired=FALSE,var.equal = TRUE)$conf
t.test(g2,g1,paired=TRUE)$conf
num<-(15.34^2/8+18.23^2/21)^2
den<-15.34^4/8^2/(8-1)+18.23^4/21^2/(21-1)
mydf<-num/den
132.86-127.44+c(-1,1)*qt(0.975,mydf)*sqrt(15.34^2/8+18.23^2/21)
10/sqrt(100)
32-30/1
2/10/4
(32-30)/(10/4)
15
qt(0.95,15)
dim(fs)
t.test(fs$sheight-fs$fheight,paired = TRUE)
t.test(fs$sheight-fs$fheight)
11.7885 * sd(fs$sheight-fs$fheight)/sqrt(1078)
mybin
mybin[7]
8
swirl()
pt(2.5,15,lower.tail = FALSE)
qnorm(.95)
qnorm(.99)
pnorm(2)
pnorm(2,lower.tail = FALSE)
mybin
pbinom(6,8,.5,lower.tail = FALSE)
pbinom(7,8,.5,lower.tail = TRUE)
ppois(9,5,lower.tail = FALSE)
t.test(1100,1)$conf.int
1100+c(-1,1)*qt(0.95,8)*(30/3)
qt(0.95,8)
qt(0.975,8)
9*.06+9*0.68
6.66/18
-2+c(-1,1)*qt(0.975,18)*0.37*(1/9+1/9)^0.5
-2+c(-1,1)*qt(0.975,18)*0.37^0.5*(1/9+1/9)^0.5
2+c(-1,1)*1.64*((99*.5+99*2)/(100+100-2))^0.5*(1/100+1/100)^0.5
(-3-1)+c(-1,1)*qt(0.95,(18-2))*((8*.1.5+8*1.8)/((9+9-2))^0.5*(1/9+1/9)^0.5
(-3-1)+c(-1,1)*qt(0.95,16*((8*.1.5+8*1.8)/((9+9-2))^0.5*(1/9+1/9)^0.5
(-3-1)+c(-1,1)*qt(0.95,16)*((8*.1.5+8*1.8)/((9+9-2))^0.5*(1/9+1/9)^0.5
(-3-1)+c(-1,1)*qt(0.95,16)*((8*.1.5+8*1.8)/(9+9-2))^0.5*(1/9+1/9)^0.5
-4+c(-1,1)*qt(0.95,16)*((8*1.5+8*1.8)/(9+9-2))^0.5*(1/9+1/9)^0.5
1100+c(-1,1)*qnorm(0.95,8)*(30/3)
1100+c(-1,1)*qnorm(0.975,8)*(30/3)
1100+c(-1,1)*qt(0.975,8)*(30/3)
qt(0.975,8)
sp <- sqrt( ((n1 - 1) * sd(x1)^2 + (n2-1) * sd(x2)^2) / (n1 + n2-2))
md <- mean(g2) - mean(g1)
semd <- sp * sqrt(1 / n1 + 1/n2)md + c(-1, 1) * qt(.975, n1 + n2 - 2) * semd
qt(0.975,16)
-2+c(-1,1)*qt(0.975,18)*0.37^0.5*(1/9+1/9)^0.5
library(swirl)
swirl()
head(pvalues)
head(pValues)
sum(pValues<0.05)
sum(p.adjust(pValues,method = "bonferroni")<0.05)
sum(p.adjust(pValues,method = "BH")<0.05)
tail(trueStatus)
table(pValues2<0.5,trueStatus)
table(pValues2 < 0.5, trueStatus)
table(pValues2 < 0.05, trueStatus)
24/524
24/500
table(p.adjust(pValues2,method="bonferroni") < 0.05, trueStatus)
table(p.adjust(pValues2,method="BH") < 0.05, trueStatus)
sum(1*1/6,2*1/6,3*1/6,4*1/6.5*1/6,6*1/6)
sum(1:6)/6
print(g2)
head(sh)
nh
median(resampledMedians)
median(sh)
sam<-sample(fh,nh*B,replace = TRUE)
resam<-matrix(sam,B,nh)
meds<-apply(resam,1,median)
median(fh)-median(meds)
sd(meds)
sd(resampledMedians)
quantile(resampledMedians,c(0.025,0.975))
quantile(meds,c(0.025,0.975))
dim(InsectSprays)
names(InsectSprays\)
names(InsectSprays)
range(Bdata$count)
range(Cdata$count)
BCcounts
group
testStat
obs<-testStat(BCcounts)
obs<-testStat(BCcounts,group)
obs
mean(Bdata$count-Cdata$count)
sample(group)
perms <- sapply(1 : 10000, function(i) testStat(BCcounts, sample(group)))
mean(perms>obs)
testStat(DEcounts,group)
perms <- sapply(1:10000, function(i) testStat(DEcounts, sample(group)))
t.test(mean=1100,sd=30)
1100+c(-1,1)*qt(0.975,8)*(30/3)
pt(2,3)
(0.44)
pt(0.44)
ppois(0.56-1,1,lower.tail=FALSE)
ppois(0.56+1,1)
ppois(0.56+1,100)
power.t.test(n = 100, delta = 0.01, sd = 0.04, type = "one.sample", alt = "one.sided")$power
power.t.test(power=0.9, delta = 0.01, sd = 0.04, type = "one.sample", alt = "one.sided")$n
basline<-c(140,138,150,148,135)
week2<-c(132,135,151,146,130)
t.test(basline,week2,paired = TRUE,alternative = "two.sided")
install.packages("UsingR")
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')
``{r setup, cache = F, echo = F, message = F, warning = F, tidy = F, results='hide'}
```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F, results='hide'}
library(UsingR); data(galton);
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
myPlot <- function(beta){
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g <- g + geom_abline(intercept = 0, slope = beta, size = 3)
mse <- mean( (y - beta * x) ^2 )
g <- g + ggtitle(paste("beta = ", beta, "mse = ", round(mse, 3)))
g
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
library(dplyr)
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
install.packages("manipulate")
library(manipulate)
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
myPlot <- function(beta){
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g <- g + geom_abline(intercept = 0, slope = beta, size = 3)
mse <- mean( (y - beta * x) ^2 )
g <- g + ggtitle(paste("beta = ", beta, "mse = ", round(mse, 3)))
g
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton)
```{r, fig.height=6,fig.width=7,echo=FALSE}
```{r, fig.height=6,fig.width=7,echo=FALSE}
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
lm1 <- lm(galton$child ~ galton$parent)
g <- g + geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2], size = 3, colour = grey(.5))
g
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')
y <- galton$child
x <- galton$parent
beta1 <- cor(y, x) *  sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
rbind(c(beta0, beta1), coef(lm(y ~ x)))
yn <- (y - mean(y))/sd(y)#Normalize
xn <- (x - mean(x))/sd(x)#normalize
c(cor(y, x), cor(yn, xn), coef(lm(yn ~ xn))[2])#derive all 3 ways
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')
```{r, fig.height=6,fig.width=6,echo=FALSE}
```{r, fig.height=6,fig.width=6,echo=FALSE}
>>>>>>> 3e5b14bbb8f101fc2a8573beb037d5f1b6f6fe47
library(UsingR)
data(father.son)
y <- (father.son$sheight - mean(father.son$sheight)) / sd(father.son$sheight)
x <- (father.son$fheight - mean(father.son$fheight)) / sd(father.son$fheight)
rho <- cor(x, y)
library(ggplot2)
<<<<<<< HEAD
= ggplot(data.frame(x = x, y = y), aes(x = x, y = y))
g = ggplot(data.frame(x = x, y = y), aes(x = x, y = y))
g = g + geom_point(size = 6, colour = "black", alpha = 0.2)
g = g + geom_point(size = 4, colour = "salmon", alpha = 0.2)
g = g + xlim(-4, 4) + ylim(-4, 4)
g = g + geom_abline(intercept = 0, slope = 1)
g = g + geom_vline(xintercept = 0)
g = g + geom_hline(yintercept = 0)
g = g + geom_abline(intercept = 0, slope = rho, size = 2)
g = g + geom_abline(intercept = 0, slope = 1 / rho, size = 2)
g
library(swirl)
install_from_swirl("Regression Models")
swirl()
plot(child~parent,galton)
plot(jitter(child,4)~parent,galton)
regrline<-lm(child~parent,galton)
abline(regrline,lwd=3,col='red')
summary(regrline)
fit<-lm(child~parent,galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals,galton$parent)
ols.ic<-fit$coef[1]
ols.slope<-fit$coef[2]
lhs-rhs
all.equal(lhs,rhs)
varChild<-var(galton$child)
varRes<-var(fit$residuals)
varEst<-var(est(ols.slope,ols.ic))
all.equal(varChild,sum(varRes,varEst))
all.equal(varChild,varRes+varEst)
efit<-lm(accel~mag+dist,attenu)
mean(efit$residuals)
cov(efit$residuals,attenu$mag)
cov(efit$residuals,attenu$dist)
cor(gpa_nor,gch_nor)
l_nor<-lm(gca_nor~gpa_norm)
l_nor<-lm(gca_nor~gpa_nor)
l_nor<-lm(gch_nor~gpa_nor)
x <- c(0.18, -1.54, 0.42, 0.95)
mean(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
R<-lm(y~x)
r$coef
R$coefficients
R<-lm(y~x+1)
R$coefficients
R<-lm(y~x-1)
R$coefficients
data(mtcars)
lm(mpg~weight)$coeff
lm(mpg~weight,mtcars)$coeff
str(mtcars)
lm(mpg~wt,mtcars)$coeff
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
(x-mean(x))/sd(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
R<-lm(y~x)
R$coefficients
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
w <- c(2, 1, 3, 1)
mean(w)*0.0025
require(datasets); data(swiss); require(GGally); require(ggplot2)
g = ggpairs(swiss, lower = list(continuous = "smooth"),params = c(method = "loess"))
g
#fit all varaiables
summary(lm(Fertility ~ . , data = swiss))$coefficients
install.packages("GGally")
#paired plots of variables in datasets
require(datasets); data(swiss); require(GGally); require(ggplot2)
g = ggpairs(swiss, lower = list(continuous = "smooth"),params = c(method = "loess"))
g
#fit all varaiables
summary(lm(Fertility ~ . , data = swiss))$coefficients
summary(lm(Fertility ~ Agriculture + factor(CatholicBin), data = swiss))$coef
library(datasets); data(swiss)
library(dplyr);
swiss = mutate(swiss, CatholicBin = 1 * (Catholic > 50))
summary(lm(Fertility ~ Agriculture + factor(CatholicBin), data = swiss))$coef
setwd("U:/General/Data_Scientist_Course/Course8_MachineLearning/Project")
#download and import datasets
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
"train.csv")
train<-read.csv("train.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
"test.csv")
test<-read.csv("test.csv")
#Remove variables that are not relevant to the analysis
train<-train[, -c(1:7)]
test<-test[, -c(1:7)]
#Remove variables with 100% missing values
train<-train[,colSums(is.na(train)) == 0]
test <-test[,colSums(is.na(test)) == 0]
#To reduce the # of variable to a manageable amount, PCA is conducted
library(caret)
PCA95<-preProcess(train[,1:52],method="pca",thresh=.95)
#Result is that 14 variables are needed to capture 95% of the variance
#Select those 14
PCA14 <- preProcess(train[,1:52],method="pca",pcaComp=14)
#Reduce to those
trainPCA <- predict(PCA14,train[,1:52])
testPCA <- predict(PCA14,test[,1:52])
library(randomForest)
m2 <- randomForest(train$classe ~ ., data=trainPCA)
m2 <- randomForest(train$classe ~ ., data=trainPCA,do.trace=F)
train<-read.csv("train.csv")
test<-read.csv("test.csv")
train<-train[, -c(1:7)]
test<-test[, -c(1:7)]
#Remove variables with 100% missing values
train<-train[,colSums(is.na(train)) == 0]
test <-test[,colSums(is.na(test)) == 0]
#To reduce the # of variable to a manageable amount, PCA is conducted
library(caret)
PCA95<-preProcess(train[,1:52],method="pca",thresh=.95)
#Result is that 14 variables are needed to capture 95% of the variance
#Select those 14
PCA14 <- preProcess(train[,1:52],method="pca",pcaComp=14)
PCA14$rotation
#Reduce to those
trainPCA <- predict(PCA14,train[,1:52])
testPCA <- predict(PCA14,test[,1:52])
m2 <- randomForest(train$classe ~ ., data=trainPCA,do.trace=F)
library(randomForest)
m2 <- randomForest(train$classe ~ ., data=trainPCA,do.trace=F)
trainPCA <- predict(PCA14,train[,1:52])
